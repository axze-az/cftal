\documentclass[10pt,a4paper,final,oneside]{article}
\usepackage{amsmath}
\usepackage[utf8]{inputenc}
\usepackage{color}
\usepackage[final]{listings}
\usepackage[bookmarks=true,hidelinks]{hyperref}
\numberwithin{equation}{subsection}


\lstset{
  % change this to 11 later
  language={C++},
  basicstyle=\footnotesize,
  keywordstyle=\bfseries\color{blue},
  identifierstyle=\color{black}
}

\begin{document}
\title{cftal - yet another short vector library}
\author{axel zeuner\footnote{I am not a native english speaker, corrections are welcome, axel.zeuner@gmx.de}}
\maketitle
\setcounter{tocdepth}{4}
\setcounter{secnumdepth}{3}

\tableofcontents

\section{Introduction}
\label{sec:introduction}

The cftal library contains a C++11(+) short vector implementation. A
short vector is vector with up to 64 elements.

The design principles of the short vector library are
templates, recursion and specialization:
\begin{itemize}
\item A vector of length $N$ contains two vectors of length $N/2$
  (this also forces the library to vector lengths of powers of 2).
\item The vectors of length $1$ are specialized
\item Vectors with the same length as hardware vector implementation
  for different system may be specialized.
\item Partial and full specialization may be used to allow the
  selection of special intrinsic functions for hardware vector
  implementations
\item Expression templates are used for operations on the short
  vectors. This allows the filtering for special operation patterns,
  for instance generation of fused multiply and add intrinsics from
  expressions like $ c= a* b + c $. More optimizations are possible,
  but not (yet) implemented.
\end{itemize}

\section{Base components}
\label{sec:base}

All short vectory library code is grouped around the recursive template
class
\begin{lstlisting}
template <typename _T, std::size_t _N> vec<_T, _N>;
\end{lstlisting}
where $\_T$ is the type and $\_N$ the number of the elements of the
vector. The partial specialized template class
\begin{lstlisting}
template <typename _T> vec<_T, 1>;
\end{lstlisting}
is defined to stop the recursion.
%
A vector of length $N$ may be constructed from
\begin{itemize}
\item a scalar, initializing every vector element to the scalar,
\item two vectors of length $N/2$ with exception of length $1$,
\item an initializer list, initializing excess elements in the
  vector from the last element of the initializer list
\end{itemize}
A vector may be split into low and high halfes using the functions
\begin{lstlisting}
template <typename _T, std::size_t _N>
const typename vec<_T, _N>::half_type&
low_half(const vec<_T, _N>& v);

template <typename _T, std::size_t _N>
const typename vec<_T, _N>::half_type&
high_half(const vec<_T, _N>& v);
\end{lstlisting}
The most arithmetic operations are defined on all types of vectors.
Logical operations on vectors return either a bit mask or a vector of
the same type as the operands. The choice of a bit mask (a vector of
bits) or the same type depends on the machine -- machines with
hardware bit mask should use a bit mask (this means, that on x86 the
type of the result of logical operations depends on the existence of
the 512 bit AVX vector extensions). \\[10pt]
%
A large number of functions exist only for vector classes of the
two floating point types double (binary64) and float (binary32).\\
The file \texttt{include/cftal/vec\_double\_n.h} declares the functions
for vectors of binary64 and also contains inline implementation of
the most functions, the file \texttt{include/cftal/vec\_float\_n.h} contains
the corresponding float functions and their implementations.

\section{Vector Math Library}
\label{sec:vec_math_lib}

This section contains the description of the algorithms used in the
vector math library. The implemented algorithms are the same or
heavily based on the algorithms used in the sun math
library. Therefore the copyright of the sun libm source codes
\small
\begin{verbatim}
/*
 * ====================================================
 * Copyright (C) 2004 by Sun Microsystems, Inc. All rights reserved.
 *
 * Permission to use, copy, modify, and distribute this
 * software is freely granted, provided that this notice
 * is preserved.
 * ====================================================
 */
\end{verbatim}
\normalsize
applies.\\[10pt]
%
Functions for vector lengths from 1 to 8 for binary64 and binary32 are implemented as direct instantiations of the template code in
\lstinline{cftal/math/elem_func.h}
and in \lstinline{cftal/math/spec_func.h}
using the macros in
\lstinline{src/vec_def_math_functions.h}.
The binary32 functions are additionally instantiated for vector length 16.
The implementation of the instantiated functions is located in
\lstinline{cftal/math/elem_func_core_f64.h} and
\lstinline{cftal/math/elem_func_core_f32.h}.
%%
Vector functions with greater length are constructed using the template
\begin{lstlisting}
template <typename _T, std::size_t _N>
cftal::vec<_T, _N>
cftal::func(const vec<_T, _N>& x)
{
    vec<_T, _N> r= vec<_T, _N>(func(low_half(r)),
                               func(high_half(x)));
    return r;
}
\end{lstlisting}
recursively, where \textbf{func} is the name of the
function to construct.\\[10pt]
%
To suppress the generation of fused multiply and add instruction
in expressions like
\begin{lstlisting}
vec<double, 2> r0= a * b + c;
vec<double, 2> r1= a * b - c;
vec<double, 2> r2= c - a * b;
\end{lstlisting}
the result of the multiplication should be assigned to a (temporary)
variable
\begin{lstlisting}
vec<double, 2> t=a*b;
vec<double, 2> r1= t + c;
vec<double, 2> r2= t - c;
vec<double, 2> r3= c - t ;
\end{lstlisting}
which breaks the expression tree and avoids the generation of these
instructions.\\[10pt]
%
The polynomial approximations used in the implemented functions
are produced using sollya \cite{ChevillardJoldesLauter2010}.

\subsection{List of functions and precisions}
\label{sub_sec:func_list}
The tables in this section contain the list of implemented functions and
their measured maximium deviations against the corresponding functions
from the mpfr library. The column about faithfully rounding describes if faithfully rounding was observed using a test data set.
All test programs use the routines from The GNU MPFR library, see
\cite{mpfr}, \cite{Fousse:2007:MMB:1236463.1236468}, as reference
implementation.
%
%Functions with names like native\_xxx implement the function xxx with
%reduced precision or range or both.
%
\subsubsection{support functions}
\begin{tabular}{ | p{2.0cm} | p{2.0cm} | p{2.0cm} | p{2.0cm} | p{2.0cm} |}
    \hline
     &
    \multicolumn{2}{| p{4.0cm} |} {\center{binary64}} &
    \multicolumn{2}{| p{4.0cm} |} {\center{binary32}} \\
    \hline
    function & $\Delta$ ulp & (prob.) faithfully &
          $\Delta$ ulp & (prob.) faithfully \\
    \hline
    ldexp & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    frexp & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    ilogb & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    isnan & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    isinf & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    isfinite & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    copysign & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    abs/fabs & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    floor & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    ceil & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    trunc & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
    rint & $\pm$ 0 & n/a  & $\pm$ 0 & n/a \\
    \hline
\end{tabular}\\[10pt]
All these functions behave as their counterparts in the std namespace.

\subsubsection{power functions}
\begin{tabular}{ | p{2.0cm} | p{2.0cm} | p{2.0cm} | p{2.0cm} | p{2.0cm} |}
    \hline
     &
    \multicolumn{2}{| p{4.0cm} |} {\center{binary64}} &
    \multicolumn{2}{| p{4.0cm} |} {\center{binary32}} \\
    \hline
    function & $\Delta$ ulp & (prob.) faithfully &
          $\Delta$ ulp & (prob.) faithfully \\
    \hline
    rsqrt & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    cbrt & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    pow & $\pm$ 2?? & ??  & $\pm$ 2?? & ?? \\
    \hline
\end{tabular}\\[10pt]
The sqrt function uses the hardware instruction provided by modern processors.
The precision of the pow function in more common areas, for instance in
$(0, 100), (0, 100)$ is better than mentioned in the table.

\subsubsection{elementary functions and erf}
\begin{tabular}{ | p{2.0cm} | p{2.0cm} | p{2.0cm} | p{2.0cm} | p{2.0cm} |}
    \hline
     &
    \multicolumn{2}{| p{4.0cm} |} {\center{binary64}} &
    \multicolumn{2}{| p{4.0cm} |} {\center{binary32}} \\
    \hline
    function & $\Delta$ ulp & (prob.) faithfully &
          $\Delta$ ulp & (prob.) faithfully \\
    \hline
    exp & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    expm1 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    exp2 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    exp2m1 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    exp10 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    exp10m1 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    sinh & $\pm$ 1 & n  & $\pm$ 1 & n \\
    \hline
    cosh & $\pm$ 1 & n  & $\pm$ 1 & n \\
    \hline
    tanh & $\pm$ 1 & n  & $\pm$ 1 & n \\
    \hline \hline
    log & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    log1p & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    log2 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    log10 & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline \hline
    sin & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    cos & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    tan & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline \hline
    asin & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    acos & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    atan & $\pm$ 1 & y  & $\pm$ 1 & y \\
    \hline
    atan2 & $\pm$ 1 & ?  & $\pm$ 1 & ? \\
    \hline \hline
    asinh & $\pm$ 1 & n  & $\pm$ 1 & n \\
    \hline
    acosh & $\pm$ 1 & n  & $\pm$ 1 & n \\
    \hline
    atanh & $\pm$ 1 & n  & $\pm$ 1 & n \\
    \hline \hline
    erf & $\pm$ 1 & y  & $\pm$ 1 & n \\
    \hline
\end{tabular}\\[10pt]
The function exp2m1 and expm10 calculate $2^x-1$ and $10^x-1$ respectively.


\subsection{Algorithms}
In the subsections below the notation $ lg(x) = \log_{10}(x)$,
$ ld(x) = \log_2{(x)} $ and $ \log{(x)} = \log_e{(x)} $ is used. A double pair
or double double variable is a floating point expansion of the form
\[
    (x_h, x_l) = x_h + x_l \, |x_l| < \frac{1}{2} |x_h^{-n}|
\]
where n is the number of mantissa bits in the floating variable.
\subsubsection{exponential functions}
\label{sub_sec:expxxx}

The exponential functions use additive argument reduction.
They use common code for the calculation of the exponential function
after reduction and scaling of the reduced argument for bases not
equal to $e$.

\paragraph{argument reduction for exp(x) and expm1(x)}

    The input argument is split into
    \begin{equation}
        x = k \times \log{(2)} + r_h + r_l, \;
        |r_h +r_l| \le \frac{1}{2} \log{(2)}
    \end{equation}
    whith $r = r_h + r_l$.
    After the determination of
    \[
        k = x \times \frac{1}{\log{(2)}}
    \]
    the values $r_h$ and $r_l$ can be calculated as
    \[
       \begin{aligned}
       hi &= x - k \times LN2_{HI} \\
       r_h &= hi - k \times LN2_{LO} \\
       dx & = hi - r_h \\
       r_l &= dx - k \times LN2_{LO}
       \end{aligned}
    \]
    where $LN2_{HI}$ and $LN2_{LO}$ are Cody and Waite constants for $\log(2)$
    and the expressions are organized to allow utilization of fma operations.

\paragraph{argument reduction for exp2(x) and exp2m1(x)}

    The calculations of exp2(x) and exp2m1(x) use the
    identity
    \begin{equation}
        2^x + C = e^{x \times \log{(2)}} + C
    \end{equation}
    for the reduced arguments.
    The reduced argument $ r_0 $ satisfies
    \begin{equation}
        x = k + r_0, \;
        |r_0| \le \frac{1}{2}
    \end{equation}
    After the determination of
    \[
        k = \text{rint}(x)
    \]
    the value $r_0$ is calculated as
    \[
       r_0 = x - k
    \]
    and then scaled
    \[
       (r_h, r_l) = r_0 - k \times (LN2_{HI}, LN2_{LO})
    \]
    to allow the calculation of $ 2^x + C $ as $ 2^k \times e^{r_h+ rl} + C$.
    The constants $LN2_{HI}$ and $LN2_{LO}$ are the two parts of a double double number for $\log{(2)}$.

\paragraph{argument reduction for exp10(x) and expm10m1(x)}

    The calculations of exp2(x) and exp2m1(x) use the
    identity
    \begin{equation}
        10^x + C = e^{x \times \log{(10)}} + C
    \end{equation}
    for the reduced arguments. A  double double pair $ r_0= r_{0h} + r_{0l} $ is constructed which satisfies
    \begin{equation}
        x = k \times \log_{10}{(2)} + r_{0h} + r_{0l}, \;
        |xr_h +xr_l| \le \frac{1}{2} \log_{10}{(2)}
    \end{equation}
    After the determination of
    \[
        k = x \times \frac{1}{\log_{10}{(2)}}
    \]
    the values $r_{0h}$ and $r_{0l}$ can be calculated as
    \[
       \begin{aligned}
       hi &= x - k \times LG2_{HI} \\
       r_{0h} &= hi - k \times LG2_{LO} \\
       dx & = hi - r_{0h} \\
       r_{0l} &= dx - k \times LG2_{LO}
       \end{aligned}
    \]
    where $LG2_{HI}$ and $LG2_{LO}$ are Cody and Waite constants for
    $\log_{10}{(2)}$
    and the expressions are organized to allow utilization of fma operations.
    The reduced argument is then scaled
    \[
        \begin{aligned}
        (r_h, r_l) &= r_{0h} - k \times (LN10_{HI}, LN10_{LO}) \\
        r_l &= r_l + r_{0l} \times LN10_{HI}
        \end{aligned}
    \]
    to allow the calculation of $ 10^x + C $ as $ 2^k * e^{r_h+ rl} + C$.
    The constants $LN10_{HI}$ and $LN10_{LO}$ are the two parts of a double double number for $\log(10)$.


\paragraph{approximation of $e^r$ in [$-\log(2)/2, \log(2)/2$]}

    The function $y = e^r$ is approximated as
    \begin{equation}
        y = \sum_{i=0}^{N}c_i r_h^i
    \end{equation}
    with $N=13$ for binary64 operands and $N=7$ for binary32 operands.
    The summations with index 0 and 1 are done in higher precision
    using an error free transformation of the horner's scheme
    (also known as Compensated Horner Scheme), see \cite{Graillat05compensatedhorner}) to achieve
    faithful rounded results.
    %
    Using the identity for the exponential function
    \[
        \begin{aligned}
        e^{r_h+r_l} - 1 &=
            e^{r_h} \times e^{r_l} - 1 \\
                        &=
            e^{r_h} \times \big( 1+ r_l + \frac{r_l^2}{2} + \dots \big) - 1 \\
                        &=
            e^{r_h} -1 + \big( e^{r_h} \times r_l +
                                e^{r_h} \times \frac{r_l^2}{2} +
                                \dots
                         \big) \\
                        &=
            e^{r_h} -1 + \big( r_l + (e^{r_h}-1) \times r_l +
                                e^{r_h} \times \frac{r_l^2}{2} +
                                \dots
                         \big) \\
        \end{aligned}
    \]
    and supressing all terms with higher powers of $r_l$ as 1 one yields
    \begin{equation}
        \begin{aligned}
        e^{r_h+r_l} - 1 & \approx
            (e^{r_h} -1) + (r_l + (e^{r_h}-1) \times r_l)
        \end{aligned}
    \end{equation}
    as error correction formula.

\paragraph{back scaling}
    Scaling back for all functions of the from $b^x$ is done
    as
    \begin{equation}
        e ^ x = 2^k * e^{(r_h + r_l)}
    \end{equation}
    with special care for border cases of k.\\[10pt]
    %
    Scaling back for all functions of the form $b^x-1$ uses
    with $ s  = \frac{1}{2} k $ one step of the error free
    transformation of the horner's scheme
    \begin{equation}
        \begin{aligned}
            (e^{\frac{1}{2}x_h}-\frac{1}{2}, e^{\frac{1}{2}x_l}-\frac{1}{2})
                &=
                    2^s \times (e^{r_h},e^{r_l}) - \frac{1}{2}\\
                &=
                    (0.5 \times 2^k) \times (e^{r_h},e^{r_l}) - \frac{1}{2}
        \end{aligned}
    \end{equation}
    for a polynomial of first order in $2^s$ and then combines the two parts
    of $ e^{\frac{1}{2}x} - \frac{1}{2} $
    \begin{equation}
        \begin{aligned}
        t &= 2 \times ( e^{\frac{1}{2}x_h} -\frac{1}{2} ) \\
        e ^ x -1 &= t + 2 \times (e^{\frac{1}{2}x_l} -\frac{1}{2})
        \end{aligned}
    \end{equation}
    into the final result. This rather complicated approach avoids the
    handling of different cases for k, especially overflows during the
    calculation.

\subsubsection{hyperbolic functions}

The hyperbolic function sinh(x), cosh(x) and tanh(x) are approximated as
described by Manos et al. in their report \cite{manos1972constrained}.

%
%TODO
%
\subsubsection{power functions}

The cbrt function ($ y=\sqrt[3]{x} $) was implemented as discussed by
W. Kahan in \cite{Kahan1991}.\\[10pt]
%
Inverse square root, to calculate $ y = 1/\sqrt{x} $ one may use the
iteration
\begin{equation}
    \begin{aligned}
        y &= y_n \left( 1+ ( x y_n^2 -1) \right)^{-1/2}
            = y_n \sum_{k=0} ({{-1/2}\atop{k}}) (x y_n^2 -1)^k \\
            & = y_n \left( 1 + \frac{1}{2} (1 - x y_n^2 ) +
            \frac{3}{8}(1- x y_n^2)^2 +
            \frac{5}{16} (1-x y_n^2)^3 \dots
            \right)
    \end{aligned}
\end{equation}
with a higher convergence rate than the newton raphson method.\\[10pt]
\subsubsection{logarithm functions}
%
The logarithm function is calculated using
\begin{equation}
    \begin{aligned}
        \log(x) &= \log(2^k \times x_r)  \\
                &= \log(2^k) + \log(x_r)  \\
                &= k \times \log(2) + \log(x_r)
  \end{aligned}
\end{equation}
and the right choice of k enforces
$ \frac{\sqrt{2}}{2} \le x_r \le \sqrt{2} $.
This may done using standard function \mbox{frexp(x, \&k)} and doubling
the resulting mantissa and decrementing k by one if the mantissa is
smaller then $\frac{\sqrt{2}}{2}$.\\[10pt]
%
The logarithm of the reduced value $x_r$ is then calculated using the
identity
\begin{equation}
  \log(x) = \log \left( \frac{1+s}{1-s} \right) = \log(1+s) - \log(1-s)
\end{equation}
with $  s= \frac{x-1}{x+1} $.
The taylor series of the right side of the equation above:
\begin{displaymath}
\log(x) = 2s
\left( 1 + \frac{1}{3} s^2 + \frac{1}{5} s^4 + \frac{1}{7} s^6 + \dots  \right)
\end{displaymath}
is approximated by a polynomial of order 11 for binary32 and 19 for binary64.
%
The functions log2(x) and log10(x) are calculated as given by their definition
\[
    \begin{aligned}
        \log_{10}(x) &= \frac{\log(x)}{\log(10)}\\
        \log_{2}(x) &= \frac{\log(x)}{\log(2)}
    \end{aligned}
\]
from log(x) with higher precision with special handling for the integer part.

\subsection{All other functions}

    All other elementary functions use the algorithms from the sun
    math library. The polynomial approximations are recalculated
    using sollya.

\subsection{TODO}
    $[0,\,\frac{1}{2}log(2)]$:

    as

    \begin{equation}
        \begin{aligned}
            e^{r} &= 1 + r + \frac{r^2}{2} + \frac{r^3}{6} + \dots \\
                  &\approx  1 + \frac {2r} {2 -r + r^2 P(r^2)} \\
        \end{aligned}
    \end{equation}

    To obtain higher precision the last equation is rewritten as
    \begin{equation}
        e^{r} \approx  1 + r + \frac{r [r -r^2 P(r^2)]} {2-[r-r^2P(r^2)]}
    \end{equation}

    Furthermore a correction term
    \[
        c = (r_h - r) - r_l
    \]
    is calculated.

    Using the taylor series of $e^{r+c}-1$ for $c \ll r$ and and truncating
    higher order terms
    \begin{equation}
        \label{equ:expm1-taylor}
        \begin{aligned}
            e^{r+c}-1 &= (e^r-1) + e^r\,c  + \frac{e^r c^2}{2} + \dots \\
                      &= (e^r-1) + e^r\,c  + \dots \\
                      &= (e^r-1) + (1+r+\frac{r^2}{2}+\dots)\,c + \dots \\
                      &= (e^r-1) + (c + r\,c + \dots) + \dots \\
                      &\approx (e^r-1) + c + r\,c
        \end{aligned}
    \end{equation}
    $e^{r+c}$ is calculated as
    \begin{equation}
        \begin{aligned}
            s &= \frac{r [r -r^2 P(r^2)]} {2-[r-r^2P(r^2)]} \\
            s &= s + r*c \\
            s &= s + c \\
            s &= s + r \\
            e^{r+c} &= s + 1
        \end{aligned}
    \end{equation}


    The order of the minimax polynomial $P(r^2)$ is 10 for double precision.

 The function value of the reduced argument is scaled back using
    \begin{equation}
        e^x = 2^k \times e^r
    \end{equation}
    to obtain the result.



double argument identity for exponential functions:
\[
    \begin{aligned}
        (b^x-1)^2 &= b^{2x} - 2b^x + 1 \\
        b^{2x}    &= (b^x-1)^2 + 2b^x - 1 \\
        b^{2x} -1 &= (b^x-1)^2 + 2b^x -2 \\
                  &= (b^x-1)^2 + 2(b^x-1)
    \end{aligned}
\]

argument transformation from $ [a, b] $ into interval $ [-1, 1] $:
\[
    \tilde x = \frac{1}{2} \big[(b-a) x + a +b \big]
\]
\\[10pt]
mapping of the interval $ [0, \infty] $ to the interval $ [-1, 1] $ for
calculation of the error function
\[
    \tilde x = \frac{x-k}{x+k}
\]
with $ k = 3.75 $.


\subsection{expm1}
\label{sub_sec:expm1}
The function expm1(x) calculates $ e^x-1 $.

\subsubsection{precision}
The functions have an error of $ \pm 1\, ulp$.

\subsubsection{implementation}
\begin{itemize}
\item Argument reduction

    Reduce $x$ to $r$ so that $ |r| \le \frac{1}{2} log(2) $
    \begin{equation}
        x = k \times log(2) + r, \; |r| \le \frac{1}{2} log(2)
    \end{equation}
    $r$ is split into two values for calculation purposes
    \[
       \begin{aligned}
       r_h &= x - k \times LN2_{HI} \\
       r_l &= k \times LN2_{LO} \\
       r &= r_h - r_l
       \end{aligned}
    \]
    where $LN2_{HI}$ and $LN2_{LO}$ are Cody and Waite constants for $log(2)$.
    Furthermore a correction term
    \[
        c = (r_h - r) - r_l
    \]
    is calculated.

\item Approximation of $e^r-1$ by a special rational function on the interval
    $[0,\,\frac{1}{2}log(2)]$:
    \[
        r \frac{e^{r}+1}{e^{r}-1} =
        2+\frac{r^2}{6}-\frac{r^4}{360}+\cdots
    \]

\end{itemize}

\subsection{exp2}
The function exp2(x) calculates $ 2^x $.

\subsection{pow}
The function pow(x,y) calculates $ x^y $.

\subsubsection{precision}
The functions have an error of $ \pm x\, ulp$.

\subsubsection{implementation}
\begin{itemize}
\item argument reduction
    \begin{equation}
        \begin{aligned}
        x^y &= (2^{k_x} \times m_x) ^ {2^{k_y} \times m_y} \\
        log(x^y) &= y*log(x) \\
                 &= (2^{k_y} log(m_x) + log(2) k_x 2^{k_y}) m_y \\
                 &= (log(m_x) + log(2) k_x) 2^{k_y} m_y \\
        \end{aligned}
    \end{equation}
\end{itemize}

\subsection{cbrt}

\subsection{asinh}
\label{sub_sec:asin}
The function asin(x) calculates $ asinh(x) $.

\subsubsection{precision}
The functions have an error of $ \pm 2\, ulp$.

\subsubsection{implementation}
\begin{itemize}
\item Argument reduction
    \[
        \begin{aligned}
          asinh(x) &= log(x+\sqrt{1+x^2}) \\
                   &= log(x+\sqrt{1-x}\sqrt{1+x}) \\
          asinh(u) + asinh(v) &= asinh(u\sqrt{1+v^2} + v\sqrt{1+u^2}) \\
          asinh(x) + asinh(x) &= asinh(x\sqrt{1+x^2} + x\sqrt{1+x^2}) \\
                              &= asinh(2\,x\sqrt{1+x^2}) \\
          x &= 2^k\times r
        \end{aligned}
    \]
\end{itemize}

\bibliographystyle{plain}
\bibliography{cftal-doc}

\end{document}

